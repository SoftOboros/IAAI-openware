# **Comparative Analysis of Analog Encoding Techniques in In-Memory Computing Architectures**

## **1\. Introduction: The Analog Renaissance in the Era of Data-Centric Computing**

The trajectory of modern computing has reached a critical inflection point where the traditional boundaries of digital processing are increasingly becoming impediments to performance rather than enablers. For decades, the Von Neumann architecture—defined by the distinct physical separation of the central processing unit and the memory hierarchy—has served as the bedrock of general-purpose computing. This paradigm relies on the continuous shuttling of data between storage elements and arithmetic logic units (ALUs), a process that was negligible when processor clock speeds and memory bandwidths were roughly commensurate. However, the advent of data-intensive workloads, particularly Deep Neural Networks (DNNs) and Large Language Models (LLMs), has exacerbated the "memory wall" to an untenable degree.  
In contemporary deep learning workloads, the energy cost of moving data often exceeds the energy cost of the mathematical operations themselves by several orders of magnitude. For instance, fetching a single operand from off-chip DRAM can consume upwards of 100 times more energy than performing a floating-point multiplication on that operand.1 As neural networks scale into the billions of parameters—as seen with modern transformer architectures—the sheer volume of synaptic weights that must be fetched for every inference pass creates a massive latency and energy bottleneck. Digital accelerators, including Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), attempt to mitigate this through massive on-chip caches and high-bandwidth memory (HBM), yet they remain fundamentally bound by the necessity to move data to the compute engines.  
Analog In-Memory Computing (AIMC) has emerged as a transformative architectural alternative designed to dismantle this bottleneck. By embedding the computational primitives directly within the memory array itself, AIMC eliminates the need to move weight data. Instead of fetching weights to an ALU, the memory array *becomes* the ALU. This is achieved by exploiting fundamental physical laws: Ohm’s Law ($I \= V \\cdot G$) allows for parallel multiplication of input voltages and stored conductances, while Kirchhoff’s Current Law ($I\_{total} \= \\Sigma I$) enables the instantaneous summation of these products along the bitlines.2 The result is a massively parallel matrix-vector multiplication (MVM) engine that operates with near-zero weight movement energy.  
However, the efficacy of this analog paradigm is inextricably linked to the mechanisms of encoding. Unlike digital systems that operate on robust, discrete binary states (0 and 1), analog computing relies on the manipulation of continuous physical quantities—voltage amplitude, pulse duration, electrical charge, and resistance. The precision, linearity, and robustness of the system depend entirely on how accurately digital information can be translated into these analog domains (Input Encoding), stored within the physical device (Weight Encoding), and retrieved for subsequent processing (Output Encoding).  
This report presents an exhaustive comparative analysis of these analog encoding techniques. It dissects the engineering trade-offs between speed, density, and precision across various modulation schemes and memory technologies. We examine the tension between Pulse Width Modulation (PWM) and Pulse Amplitude Modulation (PAM), evaluate the specific constraints imposed by Resistive RAM (ReRAM), Phase-Change Memory (PCM), Flash, and SRAM, and analyze the critical interface bottlenecks introduced by data conversion circuitry. Through this detailed exploration, we aim to provide a nuanced understanding of the design space for next-generation analog AI accelerators.

## **2\. Input Activation Encoding: The Digital-to-Analog Interface**

The first stage of any analog in-memory compute operation is the conversion of digital input activations—generated by the host processor or the preceding neural network layer—into analog signals capable of driving the memory array. This process, known as input encoding, dictates the linearity of the computation, the complexity of the peripheral circuitry, and the overall throughput of the accelerator. The two primary methodologies dominating this space are Pulse Amplitude Modulation (PAM) and Pulse Width Modulation (PWM), each offering distinct advantages and suffering from unique physical limitations.

### **2.1 Pulse Amplitude Modulation (PAM): The Throughput-First Approach**

Pulse Amplitude Modulation represents the most intuitive translation of digital values into the analog domain. In a PAM scheme, the magnitude of the input vector is encoded directly into the voltage level applied to the wordline of the memory array. For a system with $k$-bit input precision, the peripheral Digital-to-Analog Converters (DACs) must be capable of generating $2^k$ distinct voltage levels.  
The primary allure of PAM lies in its throughput potential. In an ideal PAM architecture, the entire multi-bit input vector is applied simultaneously across all rows, and the resulting current is integrated over a single, fixed read window ($T\_{read}$). This theoretically allows for single-cycle execution of a matrix-vector multiplication, regardless of the input bit precision.5 For latency-critical applications where minimizing the "time-to-solution" is paramount, PAM offers an unmatched theoretical speed advantage.  
However, the practical implementation of PAM is fraught with severe engineering challenges that scale poorly with precision. The most significant of these is the stringency of the Signal-to-Noise Ratio (SNR) requirements. To distinguish between 256 voltage levels (8-bit precision) within the limited voltage headroom of modern CMOS processes (typically roughly 0.8V to 1.0V), the voltage step size becomes extremely small—often in the range of a few millivolts. This leaves the system highly vulnerable to noise sources such as thermal noise, power supply ripple, and electromagnetic interference.7  
Furthermore, PAM is acutely sensitive to the non-idealities of the interconnects within the crossbar array, specifically the "IR drop" phenomenon. As current flows through the metal wires of the wordlines and bitlines, the parasitic resistance of the wire causes a voltage drop. In a PAM scheme, where the information is carried in the voltage magnitude, this drop effectively corrupts the input data. A memory cell located far from the row driver will physically receive a lower voltage than a cell near the driver, leading to a systematic error in the multiplication result ($I\_{cell} \\neq V\_{DAC} \\cdot G\_{cell}$). Mitigating this requires expensive compensation circuits or heavy metal routing, which reduces the array density.4  
Additionally, PAM relies on the assumption that the memory device behaves as a linear resistor ($I \\propto V$). Many emerging non-volatile memory technologies, such as ReRAM and Flash (in subthreshold), exhibit highly non-linear Current-Voltage (I-V) characteristics. In these devices, current often scales exponentially or effectively with voltage (e.g., $I \\propto \\sinh(V)$ or $I \\propto e^{\\kappa V}$). Applying a linear voltage mapping to a non-linear device results in severe computational distortion, necessitating complex pre-distortion look-up tables or nonlinear DACs that consume significant silicon area.10

### **2.2 Pulse Width Modulation (PWM): The Linearity-First Approach**

In response to the linearity and noise challenges of PAM, Pulse Width Modulation has emerged as the dominant encoding scheme for many emerging memory architectures. In PWM, the information is encoded not in the voltage amplitude, which remains constant, but in the temporal duration of the pulse.  
In a PWM scheme, the voltage applied to the wordline is binary: it is either at a fixed "Read Voltage" ($V\_{read}$) or at zero (Ground). The duration for which the voltage is high ($T\_{on}$) is proportional to the digital input value. The computation is then performed by integrating the resulting current over time on a capacitor connected to the bitline. The total accumulated charge ($Q$) is given by the integral of current over time:

$$Q \= \\int\_{0}^{T\_{on}} I(V\_{read}) \\, dt \= I(V\_{read}) \\times T\_{on}$$  
Since $V\_{read}$ is constant during the 'on' phase, the memory device operates at a single, static point on its I-V curve. This effectively bypasses the non-linearity of the device. Whether the device is a linear resistor, a diode, or a highly non-linear selector, the current $I(V\_{read})$ is constant for a given weight state. Therefore, the relationship between the input ($T\_{on}$) and the output ($Q$) is perfectly linear. This intrinsic linearity is a decisive advantage for PWM, enabling high-precision analog computing on non-ideal devices.9  
PWM also offers superior robustness against voltage noise and IR drop. While IR drop still reduces the magnitude of the current, the information—encoded in the time domain—remains largely intact. A slight reduction in current magnitude causes a gain error (scaling error) rather than the non-linear distortion seen in PAM. Furthermore, temporal signals are easier to regenerate and distribute across a chip without degradation compared to precise analog voltages.  
The "Achilles' heel" of PWM is latency. Because the pulse width must scale with the input value, the integration time increases exponentially with bit precision. A straightforward bit-parallel PWM scheme for an $N$-bit input requires a worst-case integration time of $2^N \- 1$ clock cycles. For an 8-bit input, this means the operation takes 255 cycles, compared to the single cycle of PAM. This latency penalty can create a bottleneck for high-throughput inference tasks.5

### **2.3 Hybrid and Advanced Encoding Architectures**

To navigate the trade-off between the speed of PAM and the linearity of PWM, architects have developed hybrid schemes.  
**Bit-Serial Input Processing:** This is arguably the most prevalent modern variation of PWM. Instead of generating a single long pulse proportional to the value (e.g., width 255 for value 0xFF), the inputs are fed into the array one bit at a time, from LSB to MSB (or vice versa). The array performs a binary multiplication for each bit, and the analog results are digitized and then shifted and added in the digital domain.

* *Advantage:* This reduces the time complexity from exponential ($2^N$) to linear ($N$). For 8-bit inputs, it requires 8 cycles instead of 255\.  
* *Mechanism:* This allows the use of simple 1-bit DACs (essentially drivers) while maintaining the linearity benefits of fixed-voltage reading. It shifts the complexity to the readout accumulation logic but offers a compelling balance of speed and robustness.4

**Pulse Density Modulation (PDM) / Rate Coding:** Often utilized in neuromorphic Spiking Neural Networks (SNNs), this method encodes magnitude as the frequency of fixed-width pulses. While this mimics biological neural systems and offers high noise immunity, it generally suffers from poor information density. Achieving high precision requires very long observation windows to count pulses, making it less suitable for standard deep learning acceleration where latency is a constraint, though it excels in ultra-low-power, event-driven applications.5  
**Spatial Bit-Parallelism:** In this approach, instead of extending time, the architecture uses multiple input lines (wires) to represent the bits of the input value in parallel. For example, an 4-bit input might drive 4 separate wordlines, with the weights stored in corresponding devices. This trades spatial density (more wires and DACs) for temporal speed.14

### **2.4 Comparative Summary of Input Encoding**

| Feature | Pulse Amplitude Modulation (PAM) | Pulse Width Modulation (PWM) | Bit-Serial / Hybrid |
| :---- | :---- | :---- | :---- |
| **Physical Domain** | Voltage Magnitude | Time / Pulse Duration | Time (Sequential Binary) |
| **Linearity** | Low (Limited by device I-V curve) | High (Fixed operating point) | High (Fixed operating point) |
| **Latency** | Single Cycle (Lowest) | Exponential ($2^N$ cycles) | Linear ($N$ cycles) |
| **Noise Immunity** | Low (Sensitive to voltage noise) | High (Robust to voltage noise) | High |
| **IR Drop Sensitivity** | Critical (Distorts value) | Moderate (Gain error) | Moderate (Gain error) |
| **DAC Complexity** | High (Multi-level precision required) | Low (Digital counter/timer) | Low (1-bit driver) |
| **Primary Use Case** | High-speed, lower precision, Flash | High-precision, NVM (ReRAM/PCM) | Balanced SRAM/NVM designs |

## ---

**3\. Weight Encoding and Memory Technologies**

While input encoding addresses the activation vector, the encoding of the weight matrix is determined by the physical properties of the memory technology employed. The ideal memory device for AIMC would be a linear, programmable resistor with infinite states, zero variability, and non-volatile retention. In reality, architects must contend with stochastic filament formation, crystalline drift, and subthreshold leakage. This section analyzes how weights are physically encoded across the leading memory candidates.

### **3.1 Resistive Random Access Memory (ReRAM): Filamentary Conductance**

ReRAM (or RRAM) has attracted intense interest for AIMC due to its scalability and backend-of-line (BEOL) compatibility. It operates on the principle of resistive switching in a metal-oxide dielectric (typically HfOx or TaOx).  
**Mechanism:** The application of a strong electric field creates a conductive filament of oxygen vacancies between two electrodes (SET process), switching the device to a Low Resistance State (LRS). A reverse field ruptures the filament (RESET process), reverting it to a High Resistance State (HRS). The "weight" is encoded as the conductance of this filament.4  
**Encoding Challenges:** The filamentary nature of ReRAM introduces significant stochasticity. The formation and rupture of filaments are inherently random processes, leading to cycle-to-cycle variability (during programming) and device-to-device variability (across the array). Furthermore, ReRAM devices often exhibit "relaxation," where the conductance changes slightly immediately after programming, complicating precise weight storage.  
To manage this, ReRAM-based AIMC almost universally employs Differential Encoding. Since conductance is always positive ($G \> 0$), representing the negative weights required by neural networks is impossible with a single device. Instead, two devices are allocated per weight: a "positive" conductance ($G^+$) and a "negative" conductance ($G^-$). The effective weight $W$ is the difference:

$$W \= G^+ \- G^-$$

The readout circuit subtracts the currents from the two columns ($I\_{out} \= I^+ \- I^-$). This architecture not only enables signed weights but also provides Common Mode Noise Rejection. Any global fluctuation in temperature or supply voltage that affects both devices equally is subtracted out, significantly improving the effective Signal-to-Noise Ratio (SNR).4  
**Efficiency:** Despite variability challenges, ReRAM offers exceptional energy potential. Prototypes have demonstrated efficiencies ranging from 70 to nearly 200 TOPS/W for low-precision inference tasks, benefiting from the non-volatile nature that allows power-gating of the array during idle periods.18

### **3.2 Phase-Change Memory (PCM): The Drift Conundrum**

PCM stores data in the crystalline state of a chalcogenide glass (such as Ge2Sb2Te5). It offers better endurance and more reliable analog states than many filamentary ReRAMs but introduces a unique physics-based challenge: conductance drift.  
**Mechanism:** Heat generated by current pulses transitions the material between a disordered, high-resistance amorphous state and an ordered, low-resistance crystalline state. The weight is encoded as the volume fraction of the crystalline material.2  
The Drift Problem: The amorphous phase of PCM is thermodynamically unstable. Over time, the atomic structure relaxes, causing the resistance to increase (conductance to drop) according to a power law:

$$G(t) \= G\_0 \\cdot t^{-\\nu}$$

where $\\nu$ is the drift coefficient. This drift causes the stored weights to effectively "decay" over time, leading to a progressive degradation of inference accuracy. A model programmed today might lose significant accuracy within days or weeks if uncorrected.21  
**Mitigation Strategies:** IBM Research, a leader in PCM-based AIMC, has developed sophisticated compensation techniques. These include:

1. **Multi-Device Encoding:** Using multiple PCM pairs (e.g., 4 devices) to represent a single weight, averaging out the drift errors.  
2. **Global Drift Compensation:** Monitoring the average drift of the array using reference cells and dynamically adjusting the readout gain to normalize the signal.  
3. Noise-Aware Training: Training the neural network with noise injection that mimics the drift statistics, making the model inherently robust to weight decay.  
   Through these methods, IBM has demonstrated PCM chips (such as the "Hermes" and "Fusion" prototypes) capable of maintaining ISO-accuracy (software-equivalent accuracy) on complex networks like ResNet and LSTMs over time.20

### **3.3 Flash Memory: The Mature High-Precision Candidate**

While often associated with digital storage, Flash memory (Floating Gate or Charge Trap) is a potent candidate for analog computing, utilized by companies like Mythic.  
Mechanism: Weights are stored as the amount of charge trapped on a floating gate. This charge shifts the Threshold Voltage ($V\_{th}$) of the transistor. In operation, the transistor is typically biased in the subthreshold region, where the drain current is exponentially related to the gate voltage minus the threshold voltage:

$$I\_{ds} \\propto e^{\\kappa(V\_{gs} \- V\_{th})}$$

By finely tuning the amount of trapped charge, the $V\_{th}$ can be set with extreme precision, allowing Flash to store the equivalent of 8 bits (256 levels) or more in a single cell—a density that ReRAM and PCM struggle to match reliably.25  
**Mythic's Architecture:** Mythic utilizes this property in their "Analog Matrix Processor" (M1076). They leverage 40nm embedded Flash to achieve very high weight density (up to 80 million weights on a single chip). By performing computation in the subthreshold regime, they achieve high energy efficiency. However, the exponential I-V relationship makes Flash highly sensitive to input voltage variations, necessitating robust PAM or PWM input schemes and careful temperature compensation. Mythic claims up to 25 TOPS of performance at 3-4 Watts, validating the high-density potential of Flash-based analog compute.27 The primary limitation of Flash is its low write endurance and high programming voltage requirements, making it ideal for inference-only applications where weights are static, but less suitable for on-chip training.

### **3.4 SRAM-Based Analog Computing: Stability vs. Density**

SRAM is the standard for on-chip digital memory. Adapting it for analog compute involves trade-offs between cell size and computational robustness.  
**Standard vs. Modified Cells:** Standard 6-Transistor (6T) SRAM cells are optimized for digital stability. Activating multiple rows for analog compute allows current to flow in unexpected paths, potentially flipping the stored bits ("Read Disturb"). Therefore, SRAM-CIM often requires 8T or 10T cells with decoupled read and write paths, which increases area and reduces the density advantage.30  
**Charge-Domain (Switched Capacitor) Computing:** To improve linearity, modern SRAM-CIM designs often use "Charge Domain" computing. Instead of summing currents through transistors (which are non-linear and sensitive to mismatch), these designs employ capacitors attached to the bitlines. The computation is performed by charge sharing/redistribution.

* *Mechanism:* The input activates a switch that connects a capacitor to the bitline. The voltage change on the bitline is proportional to the ratio of capacitors.  
* *Advantage:* Capacitors in CMOS processes (Metal-Oxide-Metal or MIM) are extremely linear and match well, offering far better computational precision than transistor-current summing.31

**Gain Cells (2T1C/3T1C):** Emerging research from groups like Stanford and Imec investigates "Gain Cells" using IGZO (Indium-Gallium-Zinc-Oxide) transistors. These cells function similarly to DRAM (storing data as charge on a capacitor) but include an amplification transistor for non-destructive reading. They offer a density "sweet spot" between SRAM and DRAM, with the ability to perform analog compute. Because IGZO has extremely low leakage, these cells can retain analog weights for seconds or minutes without refresh, enabling efficient "volatile" analog computing.33

## ---

**4\. Architectural Strategies for Precision: Bridging the Reliability Gap**

A fundamental tension exists in AIMC: deep learning algorithms often require 8-bit (INT8) or higher precision for optimal accuracy, but individual analog memory devices struggle to reliably store more than 2-4 bits of information due to noise and variability. Architects employ two primary strategies to bridge this gap: Multi-Level Cells (MLC) and Bit-Slicing.

### **4.1 Multi-Level Cell (MLC) Encoding**

MLC attempts to store the entire high-precision weight in a single physical device. For 4-bit precision, the device must support 16 distinct, non-overlapping conductance states.

* *The Challenge:* As the number of levels increases, the "State Margin"—the conductance gap between valid levels—shrinks. With a finite dynamic range (e.g., 1 $\\mu$S to 10 $\\mu$S), 16 levels leave very little room for error. Hardware non-idealities like Random Telegraph Noise (RTN) or thermal drift can easily cause a device's state to "wander" into the adjacent level, causing a bit error.  
* *Status:* While Flash can achieve high MLC density (proven in SSDs and Mythic's chip), emerging memories like ReRAM and PCM typically hit a reliability wall at around 2-3 bits. Beyond this, the overhead of error correction and frequent recalibration outweighs the density benefits.14

### **4.2 Bit-Slicing: The Pragmatic Compromise**

Bit-slicing decomposes a high-precision weight into multiple lower-precision components, which are stored in separate cells and recombined during or after computation. This allows the use of robust binary or ternary devices to perform high-precision math.

* **Mechanism:** An 8-bit weight might be split into four 2-bit slices. The first slice stores the 2 Least Significant Bits (LSB), and the fourth stores the Most Significant Bits (MSB). These are physically stored in different columns of the array.  
* **Recombination:** The currents from these columns are not equal in value. The current from the MSB column must be effectively multiplied by a weighting factor (e.g., $\\times 64$) relative to the LSB column. This weighting can be performed in the analog domain (by scaling the input pulse width or using different capacitor sizes) or in the digital domain (by shifting the digital output of the ADCs before adding).37  
* **Analysis:** Bit-slicing trades area for reliability. It requires more physical columns (reducing density) and more complex peripheral logic (shift-and-add circuits). However, it decouples the *system* precision from the *device* precision. Even if ReRAM cells are only reliable as binary switches, bit-slicing allows them to build a fully functional INT8 accelerator. This is currently the most robust path to commercializing ReRAM/PCM accelerators.37

## ---

**5\. The Readout Bottleneck: ADCs vs. TDCs**

Once the analog computation is complete, the accumulated result (current, charge, or time) must be converted back to digital for subsequent processing (activations, pooling, communication). This readout stage is widely recognized as the primary bottleneck in AIMC, often consuming 50% to 85% of the total energy and area of the tile.8

### **5.1 The ADC Tax**

Conventional Analog-to-Digital Converters (ADCs) scale poorly for dense memory arrays.

* **Flash ADCs:** While extremely fast, their area grows exponentially with bit precision ($2^N$ comparators). They are viable only for very low precision (1-3 bits).  
* **SAR ADCs:** Successive Approximation Register ADCs are the standard choice for medium precision (4-8 bits). They offer a reasonable balance of power and area. However, a SAR ADC is still massive compared to a nanoscale memory cell. Consequently, a single ADC must be shared (multiplexed) across many columns (e.g., 1 ADC per 64 columns).  
* **Throughput Impact:** Multiplexing creates a massive serialization bottleneck. If an array has 512 columns but only 8 ADCs, it takes 64 cycles to read out one "row" of results. This drastically undercuts the theoretical parallelism of the AIMC array.

### **5.2 Time-to-Digital Converters (TDCs): A Scalable Alternative**

For architectures using PWM or time-domain encoding, the output variable is time (a delay or pulse width). This allows the use of Time-to-Digital Converters instead of voltage ADCs.

* **Mechanism:** A TDC measures time using digital logic structures, such as a counter driven by a clock or a "delay line" (a chain of inverters). It counts how many delay stages a signal passes through before a stop signal arrives.  
* **Digital Scaling:** Unlike voltage ADCs, which struggle with the reduced voltage headroom of advanced nodes (5nm, 3nm), TDCs *benefit* from scaling. As transistors get faster, the time resolution of delay lines improves, and the circuit area shrinks. TDCs are inherently "digital-friendly" analog circuits.  
* **Efficiency:** Recent benchmarks indicate that TDCs can achieve superior energy and area efficiency for 4-6 bit precision compared to SAR ADCs. By replacing complex analog comparators with simple digital inverters and flip-flops, TDCs reduce the peripheral overhead, allowing for more TDCs per column and less multiplexing.42

### **5.3 Analog Shift-and-Add**

To further reduce the pressure on the ADC, some architectures perform the "shift-and-add" operation (required for bit-slicing) in the analog domain *before* digitization. By using switched capacitor networks to redistribute charge between the MSB and LSB columns, the system can combine the partial results into a single analog value. This means only one ADC conversion is needed for the full 8-bit weight, rather than converting each slice separately. This technique effectively moves complexity from the digital domain (post-ADC) to the analog domain (pre-ADC), saving significant conversion energy.38

## ---

**6\. Hardware Non-Idealities and System-Level Implications**

The theoretical efficiency of AIMC is tempered by the gritty reality of hardware physics. This section details the major non-idealities that designers must navigate.

### **6.1 IR Drop: The Silent Accuracy Killer**

In a crossbar array, the metal wires (Wordlines and Bitlines) have non-zero resistance. As current flows through them, a voltage drop occurs ($V \= I \\cdot R\_{wire}$).

* **Effect:** A memory cell located far from the driver sees a lower voltage than one nearby. In a PAM system, this directly corrupts the computation, as the current $I \= V\_{local} \\cdot G$ will be lower than intended. This introduces a spatial error pattern across the array.  
* **PWM Mitigation:** PWM is inherently more robust to IR drop. While the voltage drop reduces the *magnitude* of the current during the pulse, the *duration* of the pulse (which carries the information) remains largely unaffected. Although severe IR drop can degrade the edge sharpness (slew rate) of the pulse, the impact on overall linearity is significantly lower than in PAM systems.9

### **6.2 Noise Sources: RTN and Thermal Noise**

* **Random Telegraph Noise (RTN):** Caused by the trapping and detrapping of single electrons in defects within the memory device or access transistor. It manifests as discrete, random jumps in conductance (step-function noise). In ultra-scaled devices, RTN can be large enough to cause a "bit flip" between adjacent analog levels, limiting the maximum feasible MLC density.  
* **Mitigation:** Differential encoding helps reject common-mode noise but cannot eliminate intrinsic device noise. Bit-slicing improves robustness by relying on binary/ternary states, which have large noise margins, rather than fine analog levels that are easily swamped by RTN.11

### **6.3 Benchmarking Efficiency: The TOPS/W Reality**

The metric of "Tera-Operations Per Watt" (TOPS/W) is the standard for AI accelerator efficiency. However, reported numbers vary wildly based on assumptions (e.g., inclusion of peripheral power, precision used).

* **ReRAM:** Studies cite peak efficiencies of **70-195 TOPS/W** for low-precision operations.18  
* **SRAM (Binary/Ternary):** Can reach extraordinary peaks (**\~800 TOPS/W**) for 1-bit weights/activations, but efficiency drops significantly for INT8 due to the overhead of bit-serial processing and accumulation.49  
* **Flash (Mythic):** Reports system-level efficiency in the range of **\~5-10 TOPS/W** (25 TOPS at \~3-4W). While lower than raw array numbers, this represents a complete, commercial-grade system running complex 8-bit models, highlighting the "tax" of real-world overheads (ADCs, control logic, I/O).27  
* **PCM:** Efficiency is often comparable to ReRAM but constrained by the power cost of the heating elements required for programming. However, for inference (read-only), it offers high density.20

## ---

**7\. Conclusion**

The landscape of Analog In-Memory Computing is defined by a complex interplay of physics, circuit design, and architecture. No single encoding technique or memory technology has emerged as a universal panacea; rather, the optimal choice depends heavily on the specific application constraints.  
**Pulse Amplitude Modulation (PAM)** remains the king of theoretical throughput, enabling single-cycle reads. It finds its best home in **Flash-based** architectures (like Mythic's) where the mature, high-precision floating gate technology can support the stringent signal-to-noise requirements. However, the scalability of PAM is challenged by supply voltage reduction and noise in advanced nodes.  
**Pulse Width Modulation (PWM)** has established itself as the robust workhorse for emerging, non-ideal memories like **ReRAM** and **PCM**. By moving information into the time domain, PWM effectively neutralizes the non-linearity and voltage noise that plague these devices. While it incurs a latency penalty, techniques like **Bit-Serial** processing and **Time-to-Digital Converters (TDCs)** are effectively mitigating this, offering a path to high-precision, linear analog compute.  
**Bit-Slicing** has emerged as the pragmatic bridge between the desire for 8-bit precision and the reality of 1-2 bit reliable memory devices. It allows architects to build robust systems today without waiting for material scientists to perfect the "ideal" analog memory cell.  
Looking forward, the "Encoding Gap"—the energy and area cost of the DACs and ADCs—remains the primary barrier preventing AIMC from fully displacing digital accelerators. Future breakthroughs will likely prioritize **Time-Domain Computing** and **Charge-Domain** architectures that eliminate complex voltage converters entirely, integrating the analog core more seamlessly with the digital ecosystem. As these technologies mature, we can expect a continued bifurcation: Flash and SRAM dominating the high-precision, low-power inference market, while ReRAM and PCM contend for the high-density, high-performance training and server-class inference sectors.

#### **Works cited**

1. Deep Neural Network Inference with Analog In-Memory Computing \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/publications/deep-neural-network-inference-with-analog-in-memory-computing](https://research.ibm.com/publications/deep-neural-network-inference-with-analog-in-memory-computing)  
2. The Role of Phase-Change Memory in Edge Computing and Analog In-Memory Computing: An Overview of Recent Research Contributions and Future Challenges \- MDPI, accessed December 24, 2025, [https://www.mdpi.com/1424-8220/25/12/3618](https://www.mdpi.com/1424-8220/25/12/3618)  
3. Optimised weight programming for analogue memory-based deep neural networks \- PMC, accessed December 24, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9247051/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9247051/)  
4. Current Opinions on Memristor-Accelerated Machine Learning Hardware \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2501.12644v1](https://arxiv.org/html/2501.12644v1)  
5. M1076 Analog Matrix Processor Product Brief \- Mythic AI, accessed December 24, 2025, [https://mythic.ai/wp-content/uploads/2022/03/M1076-AMP-Product-Brief-v1.0-1.pdf](https://mythic.ai/wp-content/uploads/2022/03/M1076-AMP-Product-Brief-v1.0-1.pdf)  
6. Difference Between PAM, PWM and PPM (with Comparison Chart) \- Circuit Globe, accessed December 24, 2025, [https://circuitglobe.com/difference-between-pam-pwm-and-ppm.html](https://circuitglobe.com/difference-between-pam-pwm-and-ppm.html)  
7. DACs vs. PWM: Which is Better for Analog Output Control? \- Patsnap Eureka, accessed December 24, 2025, [https://eureka.patsnap.com/article/dacs-vs-pwm-which-is-better-for-analog-output-control](https://eureka.patsnap.com/article/dacs-vs-pwm-which-is-better-for-analog-output-control)  
8. Approximate ADCs for In-Memory Computing \- ResearchGate, accessed December 24, 2025, [https://www.researchgate.net/publication/383090767\_Approximate\_ADCs\_for\_In-Memory\_Computing](https://www.researchgate.net/publication/383090767_Approximate_ADCs_for_In-Memory_Computing)  
9. Pulse-Width Modulation based Dot-Product Engine for Neuromorphic Computing System using Memristor Crossbar Array, accessed December 24, 2025, [https://par.nsf.gov/servlets/purl/10089318](https://par.nsf.gov/servlets/purl/10089318)  
10. Impact of Input Encoding and ADC Resolution on Matrix-Vector Multiplication Accuracy \- IEEE Xplore, accessed December 24, 2025, [https://ieeexplore.ieee.org/iel8/10559643/10559661/10559732.pdf](https://ieeexplore.ieee.org/iel8/10559643/10559661/10559732.pdf)  
11. A Noise-Resilient Neuromorphic Digit Classifier Based on NOR Flash Memories with Pulse–Width Modulation Scheme \- ResearchGate, accessed December 24, 2025, [https://www.researchgate.net/publication/356212312\_A\_Noise-Resilient\_Neuromorphic\_Digit\_Classifier\_Based\_on\_NOR\_Flash\_Memories\_with\_Pulse-Width\_Modulation\_Scheme](https://www.researchgate.net/publication/356212312_A_Noise-Resilient_Neuromorphic_Digit_Classifier_Based_on_NOR_Flash_Memories_with_Pulse-Width_Modulation_Scheme)  
12. ASiM: Improving Transparency of SRAM-based Analog Compute-in-Memory Research with an Open-Source Simulation Framework \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2411.11022v1](https://arxiv.org/html/2411.11022v1)  
13. Understanding Pulse Modulators and Their Types \- SMART SCI & TECH, accessed December 24, 2025, [https://www.cq-smart.com/understanding-pulse-modulators-and-their-types](https://www.cq-smart.com/understanding-pulse-modulators-and-their-types)  
14. NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2505.02314v1](https://arxiv.org/html/2505.02314v1)  
15. Resistive random-access memory \- Wikipedia, accessed December 24, 2025, [https://en.wikipedia.org/wiki/Resistive\_random-access\_memory](https://en.wikipedia.org/wiki/Resistive_random-access_memory)  
16. Resistive random access memory: introduction to device mechanism, materials and application to neuromorphic computing \- PMC \- PubMed Central, accessed December 24, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10409712/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10409712/)  
17. Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2512.18459v1](https://arxiv.org/html/2512.18459v1)  
18. (PDF) 33.2 A Fully Integrated Analog ReRAM Based 78.4TOPS/W Compute-In-Memory Chip with Fully Parallel MAC Computing \- ResearchGate, accessed December 24, 2025, [https://www.researchgate.net/publication/340624693\_332\_A\_Fully\_Integrated\_Analog\_ReRAM\_Based\_784TOPSW\_Compute-In-Memory\_Chip\_with\_Fully\_Parallel\_MAC\_Computing](https://www.researchgate.net/publication/340624693_332_A_Fully_Integrated_Analog_ReRAM_Based_784TOPSW_Compute-In-Memory_Chip_with_Fully_Parallel_MAC_Computing)  
19. SPIKA: an energy-efficient time-domain hybrid CMOS-RRAM compute-in-memory macro, accessed December 24, 2025, [https://www.frontiersin.org/journals/electronics/articles/10.3389/felec.2025.1567562/full](https://www.frontiersin.org/journals/electronics/articles/10.3389/felec.2025.1567562/full)  
20. The hardware behind analog AI \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/blog/the-hardware-behind-analog-ai](https://research.ibm.com/blog/the-hardware-behind-analog-ai)  
21. Impact of Phase-Change Memory Nonidealities on Analog In-Memory Computing Deep Learning Accuracy for MRS Fall Meeting 2023 \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/publications/impact-of-phase-change-memory-nonidealities-on-analog-in-memory-computing-deep-learning-accuracy](https://research.ibm.com/publications/impact-of-phase-change-memory-nonidealities-on-analog-in-memory-computing-deep-learning-accuracy)  
22. Impact of Phase-Change Memory Drift on Energy Efficiency and Accuracy of Analog Compute-in-Memory Deep Learning Inference (Invited) \- Penn State Research Database, accessed December 24, 2025, [https://pure.psu.edu/en/publications/impact-of-phase-change-memory-drift-on-energy-efficiency-and-accu/](https://pure.psu.edu/en/publications/impact-of-phase-change-memory-drift-on-energy-efficiency-and-accu/)  
23. Reducing the Impact of Phase-Change Memory Conductance Drift on the Inference of large-scale Hardware Neural Networks for IEDM 2019 \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/publications/reducing-the-impact-of-phase-change-memory-conductance-drift-on-the-inference-of-large-scale-hardware-neural-networks](https://research.ibm.com/publications/reducing-the-impact-of-phase-change-memory-conductance-drift-on-the-inference-of-large-scale-hardware-neural-networks)  
24. An energy-efficient analog chip for AI inference \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/blog/analog-ai-chip-inference](https://research.ibm.com/blog/analog-ai-chip-inference)  
25. Flash Drives: methods and materials | Ismail-Beigi Research Group, accessed December 24, 2025, [https://volga.eng.yale.edu/teaching-resources/flash-drives/methods-and-materials](https://volga.eng.yale.edu/teaching-resources/flash-drives/methods-and-materials)  
26. Understanding Flash: Floating Gates and Wear \- flashdba, accessed December 24, 2025, [https://flashdba.com/2015/01/09/understanding-flash-floating-gates-and-wear/](https://flashdba.com/2015/01/09/understanding-flash-floating-gates-and-wear/)  
27. Products \- Mythic AI, accessed December 24, 2025, [https://mythic.ai/product/](https://mythic.ai/product/)  
28. Mythic Redefines Edge AI by Combining Analog Processing and Flash Memory \- News, accessed December 24, 2025, [https://www.allaboutcircuits.com/news/mythic-ai-redefines-edge-ai-by-combining-analog-processing-and-flash-memory/](https://www.allaboutcircuits.com/news/mythic-ai-redefines-edge-ai-by-combining-analog-processing-and-flash-memory/)  
29. M1076 Analog Matrix Processor \- Mythic AI, accessed December 24, 2025, [https://mythic.ai/products/m1076-analog-matrix-processor/](https://mythic.ai/products/m1076-analog-matrix-processor/)  
30. A review on SRAM-based computing in-memory: Circuits, functions, and applications \- Researching, accessed December 24, 2025, [https://www.researching.cn/ArticlePdf/m00098/2022/43/3/031401.pdf](https://www.researching.cn/ArticlePdf/m00098/2022/43/3/031401.pdf)  
31. Charge-Domain Static Random Access Memory-Based In-Memory Computing with Low-Cost Multiply-and-Accumulate Operation and Energy-Efficient 7-Bit Hybrid Analog-to-Digital Converter \- MDPI, accessed December 24, 2025, [https://www.mdpi.com/2079-9292/13/3/666](https://www.mdpi.com/2079-9292/13/3/666)  
32. SRAM In-Memory Computing Macro With Delta-Sigma Modulator-Based Variable-Resolution Activation \- Arizona State University, accessed December 24, 2025, [https://labs.engineering.asu.edu/mixedsignals/wp-content/uploads/sites/58/2023/12/SRAM\_IMC\_SSCL\_2023.pdf](https://labs.engineering.asu.edu/mixedsignals/wp-content/uploads/sites/58/2023/12/SRAM_IMC_SSCL_2023.pdf)  
33. Analog in-memory computing attention mechanism for fast and ..., accessed December 24, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12457188/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12457188/)  
34. Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2409.19315v1](https://arxiv.org/html/2409.19315v1)  
35. IGZO-based DRAM for energy and area-efficient analog in-memory computing \- Imec, accessed December 24, 2025, [https://www.imec-int.com/en/articles/igzo-based-dram-energy-and-area-efficient-analog-memory-computing](https://www.imec-int.com/en/articles/igzo-based-dram-energy-and-area-efficient-analog-memory-computing)  
36. In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2510.02516v1](https://arxiv.org/html/2510.02516v1)  
37. Precision of bit slicing with in-memory computing based on analog phase-change memory crossbars \- ResearchGate, accessed December 24, 2025, [https://www.researchgate.net/publication/358184090\_Precision\_of\_bit\_slicing\_with\_in-memory\_computing\_based\_on\_analog\_phase-change\_memory\_crossbars](https://www.researchgate.net/publication/358184090_Precision_of_bit_slicing_with_in-memory_computing_based_on_analog_phase-change_memory_crossbars)  
38. Analog-to-Digital Converter Design Exploration for Compute-in-Memory Accelerators \- imec Publications Repository, accessed December 24, 2025, [https://imec-publications.be/bitstreams/3f194c83-ecf8-41b2-9261-4db110ab1315/download](https://imec-publications.be/bitstreams/3f194c83-ecf8-41b2-9261-4db110ab1315/download)  
39. Precision of bit slicing with in-memory computing based on analog phase-change memory crossbars \- IBM Research, accessed December 24, 2025, [https://research.ibm.com/publications/precision-of-bit-slicing-with-in-memory-computing-based-on-analog-phase-change-memory-crossbars](https://research.ibm.com/publications/precision-of-bit-slicing-with-in-memory-computing-based-on-analog-phase-change-memory-crossbars)  
40. Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2404.06553v1](https://arxiv.org/html/2404.06553v1)  
41. Benchmarking In-memory Computing Architectures \- IEEE Xplore, accessed December 24, 2025, [https://ieeexplore.ieee.org/ielaam/8782712/9733783/9976888-aam.pdf](https://ieeexplore.ieee.org/ielaam/8782712/9733783/9976888-aam.pdf)  
42. Merits of Time-Domain Computing for VMM \- A Quantitative Comparison \- arXiv, accessed December 24, 2025, [https://arxiv.org/html/2403.18367v1](https://arxiv.org/html/2403.18367v1)  
43. A Time-to-Digital Converter for Low-Power Consumption Single Slope Analog-to-Digital Converters in a High-Speed CMOS Image Sensor \- NIH, accessed December 24, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11123234/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11123234/)  
44. An Area and Energy-Efficient SRAM Based Time \- Domain Compute-In-Memory Architecture For BNN \- IEEE Xplore, accessed December 24, 2025, [https://ieeexplore.ieee.org/document/10595875/](https://ieeexplore.ieee.org/document/10595875/)  
45. Analog-to-Digital Converter Design for Diverse Performance Computing-in-Memory Systems: A Comprehensive Review \- IEEE Xplore, accessed December 24, 2025, [https://ieeexplore.ieee.org/iel8/10410247/11095298/11006374.pdf](https://ieeexplore.ieee.org/iel8/10410247/11095298/11006374.pdf)  
46. Analog-to-Digital Converter Design Exploration for Compute-in-Memory Accelerators \- IEEE Xplore, accessed December 24, 2025, [https://ieeexplore.ieee.org/ielaam/6221038/9722756/9319225-aam.pdf](https://ieeexplore.ieee.org/ielaam/6221038/9722756/9319225-aam.pdf)  
47. Fast IR-Drop Model of Memristor Crossbars and Circuit Compensation Utilizing DTCO | Request PDF \- ResearchGate, accessed December 24, 2025, [https://www.researchgate.net/publication/392446424\_Fast\_IR-Drop\_Model\_of\_Memristor\_Crossbars\_and\_Circuit\_Compensation\_Utilizing\_DTCO](https://www.researchgate.net/publication/392446424_Fast_IR-Drop_Model_of_Memristor_Crossbars_and_Circuit_Compensation_Utilizing_DTCO)  
48. A Noise-Resilient Neuromorphic Digit Classifier Based on NOR Flash Memories with Pulse–Width Modulation Scheme \- MDPI, accessed December 24, 2025, [https://www.mdpi.com/2079-9292/10/22/2784](https://www.mdpi.com/2079-9292/10/22/2784)  
49. An In-memory-computing DNN Achieving 700 TOPS/W and 6 TOPS/mm \- Princeton University, accessed December 24, 2025, [http://www.princeton.edu/\~nverma/VermaLabSite/Publications/2019/ZhangVerma\_JETCAS2019.pdf](http://www.princeton.edu/~nverma/VermaLabSite/Publications/2019/ZhangVerma_JETCAS2019.pdf)